\section{Introduction}
	
	Despite its significance in nature, the brain is one of the least understood
	entities known to science. Current attempts to understand the brain centre on
	building models of its behaviour. Though models such as Spaun are able to
	produce high-level, realistic behaviour, they run slowly conventional
	computers taking 2.5 hours to simulate one second of neural activity
	\cite{eliasmith12}.
	
	Neural models are typically a large graph of `neurons' which are connected to
	potentially thousands of others. Signals, known as spikes, are produced and
	received between connected neurons resulting in large amounts of very small
	messages being passed between processors \cite{vainbrand11}. Because neurons
	are often cheap to simulate, typical super computers featuring powerful
	processors with comparatively limited interconnection networks perform poorly.
	
	The Blue Brain project has built a model with extremely realistic neuron
	behaviour which can exploit typical super-computer resources \cite{markram06}.
	However, models are severely limited in size to hundreds of thousands of
	neurons compared with the 85 billion in a human brain. This work instead
	focuses on the simulation of large networks of simple neurons such as Spaun.
	
	Due to the unsuitability of conventional architectures, special-purpose
	systems have been built for neural simulation. In this short report current
	attempts to overcome these limitations are described followed by an overview
	of preliminary work carried out on the SpiNNaker brain simulation
	architecture. The report concludes with the research plan proposed to develop
	this research to eventually yield an improved architecture for brain
	simulation with a focus on the topology of the interconnection network.

\section{Brain Simulators}

Others have tried to produce machines optimised for the job. Many inspired by
analogue nature of the brain replace conventional computational nodes with
analogue electronics which power-efficiently implement popular models. Many of
these have been experimental and only supported relatively small numbers of
neurons [cite some early examples] and thus have been able to simply connect
all-to-all. This really doesn't scale.

Current topologies have been developed with fairly limited scalability.

\subsection{Neurogrid}

Neurogrid features chips which can simulate 65,025 neurons simultaneously but
serialises their communication. As a result of this, the time taken to send a
message to $N$ neurons is $O(N)$. Chips are combined in a tree to build larger
systems. [Why is this limited?]

\subsection{BrainScaleS}

BrainScaleS uses a silicon wafer worth of chips in a mesh network. This network
is limited in size by silicon manufacturing process constraints and must be
extended using off-wafer connections to other wafers (proposed using 10 gbit/s
Ethernet) currently in a star network. This has obvious scaling limits.

\subsection{SpiNNaker}

Finally, the focus of my work so far: SpiNNaker has many small processors
connected via a torus network whose size is only limited by the address space
available to neurons. Network optimised for small (maybe zero-data) packets.
Unlike the first two it runs simulations in software making it far more
flexible, easier with current understanding of electronics but less power
efficient.

Here's how it looks in implemented hardware: cores, chips and boards.

\section{Preliminary Work}

Current work has focused on SpiNNaker and possible extensions to it.

\subsection{SpiNNaker Topology}

To make things practical, board-to-board links use different technology to
within-board links as hundreds of wires is impractical between boards but fine
on-board. Built a simulator which simulates a model of the network to examine
how these links affect performance. Found that these links do introduce a
significant penalty but one which is probably not an issue for current
simulation needs but may be later when latency becomes more sensitive.

\subsection{SpiNNaker Wiring}

Wiring up big machines is tough. Boards must be placed into cabinets and wires
run by hand between them. Three major constraints: cost of the cables, length to
allow tech to work fast and complexity to make it possible to wire up by hand.
Work was done to propose wiring schemes for large SpiNNaker machines of 1,200
boards.

Cost constraint solved by using the high speed links tested previously.

In SpiNNaker you have a torus meaning you get links which span one end of the
system to another. This violates the length requirement. To solve this the
system is 'folded'. This is illustrated in a ring network (figure) and easily
generalises to a torus. This removes the long wrap-around link eliminating long
wires from the system.

A folded system, when mapped into cabinets and racks, becomes very non-uniform.
Luckily it isn't as bad as it looks, especially for larger systems (see figure
of full machine) but listing every connection and expecting a human to do it
isn't viable. Regularity can be found by dividing the connections up by logical
direction and whether they leave the cabinet they're in or not. For a large
machine the number of instructions therefore drops from 3,600 (one per
connection) to 53 (with each rule being used to connect hundreds of wires).

\subsection{Small-World Super-Computers}

The connections in the brain, along with many graphs observed in nature exhibits
the 'small world' property, first described by Frigyes Karinthy in 1929, which
states that the maximum length of a path between any two nodes in a small-world
graph is very short. This idea is popularly known as the theory of six degrees
of seperation and is the basis of theoretical work carried out by Watts and
Strogatz who proposed an algorithm for generating random graphs with the
small-world property.

This work extends this model to generate torus-like networks (as in SpiNNaker)
augmented with random connections and found that an great reduction in the
number of links messages must traverse in the network was achieved for a modest
increase in the number of connections in the network.

Based on the work on wiring practicality, this system was extended to generate
networks which met the wiring practicality criteria outlined above. Despite
severe restrictions on allowed connections the new topology still resulted in
improved performance.

\section{Research Plan}

[Gantt Chart]

Initial work will focus on the development of a simulator for SpiNNaker's
interconnect topology to contribute to work comparing actual prototype SpiNNaker
hardware against various software models of its behaviour. This work is hoped
for Journal publication later this year. This work is expected to take 

With the development of a suitable simulator, this will be extended to perform
more detailed experiments on the semi-random small-world networks examined in
the preliminary work.

With the process for extending the simulator and performing comparisons proven
for the small-world network case, other less conventional topologies such as
express cubes, which offer latency advantages over standard torus networks will
be examined and compared.

Work will continue looking at the impact of 'place and route', the task of
allocating parts of a neural simulation to spinnaker's processor resources and
routing spikes between them. This will provide a further comparison to be drawn
between the topologies examined as place and route can be extremely
computationally expensive. In addition, this will consider the effect of
multicast traffic.

The penultimate stage of the project will be to compare available link
technologies. The tech used in SpiNNaker is no longer available and a
replacement is needed.

The final goal of the project will be to bring together the work to suggest a
new architecture for a next-generation SpiNNaker.

