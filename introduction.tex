\chapter{Introduction}
	
	%Some high-level motivation and context to sort of set the scene before going
	%into what other folk have done and what I'm hoping to do.
	
	Modern computer systems represent some of the most complex devices ever
	constructed. Indeed, current computer technologies have enabled everything
	from global, instantaneous communications via the internet to faster and more
	effective cancer treatments\cite{nassif}. Despite this, the brain still
	outperforms conventional digital computers at many tasks.
	
	Considerable effort has been made by researchers to understand how the brain
	works. The small-scale operation of individual neurons in the brain is now
	relatively well understood but the manner of their interactions is still not
	clear. Current efforts to understand the brain attempt to model it as a
	biological computer. Such biological computers differ greatly from digital
	computers and so simulating their behaviour using existing technology is
	challenging.
	
	Recent neural models such as the Spaun brain simulator \cite{eliasmith12}
	exhibit a remarkable range of cognitive abilities such as memory, problem
	solving and pattern recognition. As well as realistic functional behaviour,
	they also respond to failures, such as neuron death, just as real brains do
	exhibiting a gradual decline in functionality. The existence of realistic
	models means studying the properties of the brain in a highly controlled,
	observable manner as in a simulator is becoming a possibility.
	
	Unfortunately, neural models are extremely computationally expensive and run
	very slowly. Spaun, for example, takes around two and a half hours to simulate
	one second of neural activity. In order to make such models usable, ways of
	executing them faster, possibly in real time are clearly required.
	
	The SpiNNaker project is developing a specialised, massively parallel super
	computer aimed at flexibly simulating extremely large neural models in real
	time. The system is made up over one million low-powered ARM processors
	connected by a specially designed interconnection network. My research hopes
	to improve the design of interconnection networks, such as SpiNNaker's, for
	use in simulating modern neural models.
	
	\section{Parallel Computing}
		
		The race to drive up the speed of individual processor cores ground to a
		halt in the mid 2000s leaving designers to use the vast number of
		transistors made available by Moore's law to find their performance gains.
		In the years that followed multi-core processors have appeared in everything
		from microcontrollers to large super computers. Despite this widespread
		availability they represent an acute challenge to software developers to
		exploit.
		
		% Better entitled "Locality"?
		\subsection{Programming}
			
			% Hard to program parallel systems - deadlocks, locality becomes an issue -
			% things can be a long way away. Cache tries to hide this but fails to
			% scale. Other approaches might work (e.g. spinnaker has slow step with
			% communication happening before the next step, i.e. synchronous model).
			
			\begin{figure}
				\begin{subfigure}[t]{0.19\textwidth}
					\center
					\input{figures/memory-model-c}
					\caption{Basic}
					\label{fig:memory-model-c}
				\end{subfigure}
				\begin{subfigure}[t]{0.19\textwidth}
					\center
					\input{figures/memory-model-cache}
					\caption{Cached}
					\label{fig:memory-model-cache}
				\end{subfigure}
				\begin{subfigure}[t]{0.29\textwidth}
					\center
					\input{figures/memory-model-shared-memory}
					\caption{Shared}
					\label{fig:memory-model-shared-memory}
				\end{subfigure}
				\begin{subfigure}[t]{0.29\textwidth}
					\center
					\input{figures/memory-model-distributed-memory}
					\caption{Distributed}
					\label{fig:memory-model-distributed-memory}
				\end{subfigure}
				
				\caption{Memory Models}
				\label{fig:memory-model}
			\end{figure}
		
			
			Mainstream programming languages, such as C, work on a simplified model of
			a computer where there is a single memory which is uniform in access cost
			and a single processor, capable of sequentially executing instructions
			which access this memory (Figure \ref{fig:memory-model-c}).
			
			The first assumption, the uniform accessibility of memory, has been false
			for some time. As processor performance increased, memories have grown
			proportionally in size but not speed \cite{wilkes01}. As a result, CPU
			designers introduced caches between the memory and CPU implemented with
			smaller but faster memory technologies (Figure
			\ref{fig:memory-model-cache}). Modern machines may include several levels
			of caching, three in the case of typical workstations. As a result,
			programmers now had to manually keep track of what parts of memory were
			(likely to be) `local' to the processor (in the cache) and what was
			`remote' (not in the cache) in order to achieve good performance.
			
			In multi-processor systems the second assumption no longer holds. These
			machines can be divided into two broad categories: shared memory and
			distributed memory architectures.
			
			Shared memory architectures feature multiple cores which have a single
			shared, global memory accessed via a hierarchy of shared and private
			caches (Figure \ref{fig:memory-model-shared-memory}). This model is used
			by libraries such as OpenMP\cite{openmp} which extends C with the ability
			to use multiple processors simultaneously to process (distinct parts of) a
			shared dataset. Shared memory architectures require intricate mechanisms
			for ensuring that the private caches in the system remain coherent. These
			mechanisms incur performance, power and space overheads and are difficult
			to scale to large numbers of processors.
			
			Distributed memory architectures finally break the idea of a global memory
			by attaching a private memory to each processor or group of processors
			(Figure \ref{fig:memory-model-distributed-memory}). In order to access
			data stored in a `remote' memory, a processor must explicitly request it
			from that processor over some communication medium. Standards such as MPI
			(Message Passing Interface) provide interfaces for this type of
			programming. Distributed memory architectures are much more scalable as
			the lack cache coherency hardware means the amount of power and space
			required per processor scales linearly. Unfortunately, it moves the burden
			of deciding the location of data in the system to the programmer.
			
			% TODO Sum up talking about locality becoming an explicit problem
			
		\subsection{Architecture}
			
			Lots of computers must be connected together. Current styles are largely
			regular. Each offers its own advantages to different tasks. More in
			background chapter.
	
	\section{Computational Problems}
		
		Computers are faced with a wide range of problems all of which require
		different patterns of communication between the different parts of the
		program influencing the type of interconnect topology they suit.
		
		\subsection{Traditional `Scientific Computing'}
			
			Computational Fluid Dynamics (CFD).  Such algorithms such as heat
			propagation in a material. Lots of computation and very local,
			neighbour-to-neighbour communication.
		
		\subsection{Data Mining}
			
			Lots of stuff on the web. Facehole, Googlebook.
		
		\subsection{Brain Simulation}
			
			% TODO: Cite overview of computational neuroscience?
			
			The brain is an extremely powerful computer about which little is
			understood. The field of computational neuroscience hopes to bring
			understanding of the computational abilities and mechanisms of the
			brain.
			
			One approach to this problem is to try and produce simulations of models
			of the brain in order to understand and study their behaviour. These
			approaches often take the form of simulated populations of neurons, one
			of the basic building-blocks of the brain. Such models typically consist
			of a set of neurons, each connected to many other neurons.  Unlike
			digital circuits where each individual component connects to only a few
			or even one other component, neurons tend to be connected to hundreds or
			thousands of other neurons.
			
			Simulations of the brain therefore present a computational challenge as
			large amounts of communication must take place between all the neurons
			in the simulated system. Various architectures have been designed to
			solve this task and a selection are presented in Chapter
			\ref{chap:background}.
			
			% TODO: What is going on in brain simulators?
	
	\section{Architecture}
		
		Two major considerations when designing an architecture are the topology and
		way in which packets will be routed around the system.
		
		\subsection{Topology}
			
			Again?
		
		\subsection{Routing}
			
			Again?
		
		\subsection{Multicast}
			
			Quite simple, really.

